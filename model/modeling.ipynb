{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trainset과 Validation set 나누기 \n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# 원본 데이터 경로\n",
    "data_dir = r'E:\\musinsa\\data_file\\cropped_images_no_detection_copy'\n",
    "\n",
    "# 훈련 및 검증 데이터셋 경로\n",
    "train_dir = r'E:\\musinsa\\data_file\\train_product'\n",
    "val_dir = r'E:\\musinsa\\data_file\\val_product'\n",
    "\n",
    "# 검증 데이터 비율 설정 (예: 20% 검증, 80% 훈련)\n",
    "val_split = 0.2\n",
    "\n",
    "# CSV 파일 경로\n",
    "csv_file_path = r'E:\\musinsa\\image_상품전체사진수.csv'\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# 이미지 수가 12개 이상인 폴더만 필터링\n",
    "filtered_folders = df[df['ImageCount'] >= 12]['FolderName'].apply(str).tolist()\n",
    "print(f\"필터링된 폴더: {filtered_folders}\")\n",
    "\n",
    "# train과 val 디렉토리가 없으면 생성\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# 클래스별로 이미지 나누기\n",
    "for class_name in os.listdir(data_dir):\n",
    "    print(f\"현재 처리 중인 클래스: {class_name}\")\n",
    "    if class_name in filtered_folders:\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "\n",
    "        if os.path.isdir(class_path):\n",
    "            # 이미지 파일 목록 가져오기\n",
    "            images = os.listdir(class_path)\n",
    "            print(f\"이미지 파일 목록: {images}\")\n",
    "            if not images:\n",
    "                print(f\"{class_name}에 이미지가 없습니다.\")\n",
    "                continue\n",
    "\n",
    "            random.shuffle(images)  # 이미지를 랜덤하게 섞음\n",
    "\n",
    "            # 검증 데이터와 훈련 데이터로 나누기\n",
    "            split_idx = int(len(images) * val_split)\n",
    "            val_images = images[:split_idx]\n",
    "            train_images = images[split_idx:]\n",
    "\n",
    "            # 클래스별 훈련/검증 디렉토리 생성\n",
    "            train_class_dir = os.path.join(train_dir, class_name)\n",
    "            val_class_dir = os.path.join(val_dir, class_name)\n",
    "            os.makedirs(train_class_dir, exist_ok=True)\n",
    "            os.makedirs(val_class_dir, exist_ok=True)\n",
    "\n",
    "            # 훈련 데이터 복사\n",
    "            for image in train_images:\n",
    "                src = os.path.join(class_path, image)\n",
    "                dst = os.path.join(train_class_dir, image)\n",
    "                print(f\"훈련 데이터 복사: {src} -> {dst}\")\n",
    "                shutil.copy(src, dst)\n",
    "\n",
    "            # 검증 데이터 복사\n",
    "            for image in val_images:\n",
    "                src = os.path.join(class_path, image)\n",
    "                dst = os.path.join(val_class_dir, image)\n",
    "                print(f\"검증 데이터 복사: {src} -> {dst}\")\n",
    "                shutil.copy(src, dst)\n",
    "\n",
    "print(\"훈련 및 검증 데이터셋 분할 완료.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 상품 1000개 모델 학습 전  -증강 \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from rembg import remove, new_session\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# GPU가 사용 가능한지 확인하고 메모리 성장을 설정\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"사용 가능한 GPU: {gpus}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU 설정 중 오류 발생: {e}\")\n",
    "else:\n",
    "    print(\"GPU를 찾을 수 없습니다. CPU로 실행합니다.\")\n",
    "\n",
    "# 데이터셋 경로 설정\n",
    "train_dir = r'E:\\musinsa\\data_file\\train'\n",
    "val_dir = r'E:\\musinsa\\data_file\\val'\n",
    "\n",
    "# 이미지 크기와 배치 크기 설정\n",
    "img_height, img_width = 224, 224\n",
    "batch_size = 32\n",
    "session = new_session(model_name='u2net_human_seg')\n",
    "\n",
    "# 가우시안 블러 함수 정의\n",
    "def add_gaussian_blur(image):\n",
    "    image = np.array(image, dtype=np.float32)  # 이미지를 numpy 배열로 변환\n",
    "    if random.random() < 0.5:  # 50% 확률로 블러 적용\n",
    "        image = cv2.GaussianBlur(image, (5, 5), 0)  # 가우시안 블러 적용\n",
    "    return image\n",
    "# 배경 제거 함수 정의\n",
    "def remove_background(image):\n",
    "    # 일정 확률로 배경 제거 적용 (예: 50%)\n",
    "    if random.random() < 0.5:  # 50% 확률로 배경 제거 적용\n",
    "        pil_image = Image.fromarray((image * 255).astype(np.uint8))  # NumPy 배열을 PIL 이미지로 변환\n",
    "        pil_image = remove(pil_image, session=session)  # rembg를 사용해 배경 제거\n",
    "        # RGBA(4채널)를 RGB(3채널)로 변환\n",
    "        pil_image = pil_image.convert(\"RGB\")\n",
    "        return np.array(pil_image).astype(np.float32) / 255.0  # PIL 이미지를 NumPy 배열로 다시 변환하고 정규화\n",
    "    else:\n",
    "        return image  # 배경 제거 없이 원본 이미지 반환\n",
    "\n",
    "# 데이터 증강 및 전처리 설정 (증강된 이미지 생성)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=lambda img: remove_background(add_gaussian_blur(img)),  # 배경 제거 후 가우시안 블러 적용\n",
    "    rotation_range=40,                # 0~40도 범위에서 임의의 회전\n",
    "    width_shift_range=0.2,            # 수평 방향으로 최대 40% 임의 이동\n",
    "    height_shift_range=0.2,           # 수직 방향으로 최대 20% 임의 이동\n",
    "    shear_range=0.2,                  # 전단 변환 강도\n",
    "    zoom_range=0.2,                   # 확대/축소 범위 (최대 20%)\n",
    "    horizontal_flip=False,            # 수평 반전\n",
    "    vertical_flip=False,              # 수직 반전\n",
    "    fill_mode='nearest',              # 회전이나 이동으로 인해 생긴 빈 픽셀을 채우는 방법 ('nearest', 'constant', 'reflect', 'wrap')\n",
    "    brightness_range=[0.7, 1.2],      # 밝기 조정 (최소 70%, 최대 120%)\n",
    "    channel_shift_range=0.2,          # 색상 채널 이동 범위\n",
    "    rescale=1./255,                   # 픽셀 값을 0-1 범위로 정규화\n",
    "    dtype='float32'                   # 출력 데이터의 dtype 설정\n",
    ")\n",
    "# 원본 이미지에서 증강된 이미지 생성 후 총 1000개로 맞추기\n",
    "augmented_dir = os.path.join(train_dir, 'augmented')\n",
    "os.makedirs(augmented_dir, exist_ok=True)\n",
    "\n",
    "target_num_images = 200  # 각 클래스당 총 이미지 개수 (기존 이미지 + 증강 이미지)\n",
    "\n",
    "for class_dir in os.listdir(train_dir):\n",
    "    if not os.path.isdir(os.path.join(train_dir, class_dir)):\n",
    "        continue\n",
    "    \n",
    "    source_dir = os.path.join(train_dir, class_dir)\n",
    "    target_dir = os.path.join(augmented_dir, class_dir)\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    current_images = os.listdir(source_dir)\n",
    "    num_existing_images = len(current_images)\n",
    "    num_needed_images = target_num_images - num_existing_images\n",
    "    \n",
    "    if num_needed_images <= 0:\n",
    "        # 이미 1000개 이상의 이미지가 있는 경우\n",
    "        for filename in current_images:\n",
    "            img_path = os.path.join(source_dir, filename)\n",
    "            img = tf.keras.preprocessing.image.load_img(img_path, target_size=(img_height, img_width))\n",
    "            img.save(os.path.join(target_dir, filename))  # 이미지를 복사\n",
    "        continue\n",
    "    \n",
    "    # 필요한 증강 횟수를 계산\n",
    "    num_images_per_original = num_needed_images // num_existing_images\n",
    "    remainder = num_needed_images % num_existing_images\n",
    "\n",
    "    # 각 이미지에 대해 고르게 증강을 적용\n",
    "    for i, filename in enumerate(current_images):\n",
    "        img_path = os.path.join(source_dir, filename)\n",
    "        img = tf.keras.preprocessing.image.load_img(img_path, target_size=(img_height, img_width))\n",
    "        x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        x = x.reshape((1,) + x.shape)\n",
    "        \n",
    "        # 원본 이미지 복사\n",
    "        tf.keras.preprocessing.image.save_img(os.path.join(target_dir, filename), x[0])\n",
    "        \n",
    "        # 증강된 이미지 생성 (각 이미지를 고르게 증강)\n",
    "        for j in range(num_images_per_original):\n",
    "            for batch in train_datagen.flow(x, batch_size=1, save_to_dir=target_dir, save_prefix='aug', save_format='jpeg'):\n",
    "                break\n",
    "        \n",
    "        # 나머지 증강 작업을 일부 이미지에 추가로 적용\n",
    "        if i < remainder:\n",
    "            for batch in train_datagen.flow(x, batch_size=1, save_to_dir=target_dir, save_prefix='aug', save_format='jpeg'):\n",
    "                break\n",
    "\n",
    "# 원본 이미지와 증강된 이미지 모두를 포함한 훈련 데이터 로드\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255  # 정규화\n",
    ")\n",
    "\n",
    "train_ds = train_datagen.flow_from_directory(\n",
    "    augmented_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# 검증 데이터 로드\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "val_ds = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##상품 1000개 학습  \n",
    "import tensorflow as tf\n",
    "\n",
    "# GPU가 사용 가능한지 확인하고 메모리 성장을 설정\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"사용 가능한 GPU: {gpus}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU 설정 중 오류 발생: {e}\")\n",
    "else:\n",
    "    print(\"GPU를 찾을 수 없습니다. CPU로 실행합니다.\")\n",
    "\n",
    "# ResNet50 모델 로드 (사전 학습된 ImageNet 가중치 사용)\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# ResNet50 모델의 모든 층을 학습 가능하게 설정\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Sequential 모델 구성\n",
    "model = Sequential()\n",
    "\n",
    "# ResNet50 기본 모델  \n",
    "model.add(base_model)\n",
    "\n",
    "\n",
    "# 완전 연결층(Fully Conneced Layers) \n",
    "model.add(Flatten())  # 특징 맵을 1차원으로 변환 \n",
    "model.add(Dense(1024, activation='relu'))  # Fully Connected 층 추가\n",
    "model.add(Dropout(0.5))  # 과적합 방지를 위한 Dropout 층 추가 \n",
    "\n",
    "# 출력층(Output Layer)\n",
    "model.add(Dense(train_ds.num_classes, activation='softmax'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 체크포인트 콜백 설정\n",
    "checkpoint_cb = ModelCheckpoint(r'E:\\musinsa\\checkpoints\\resnet50_custom_model_checkpoint_epoch_{epoch:02d}.h5', \n",
    "                                save_best_only=True,\n",
    "                                monitor='accuracy', \n",
    "                                mode='max',\n",
    "                                verbose=1)\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    epochs=100,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[checkpoint_cb]\n",
    ")\n",
    "\n",
    "# 학습 완료 후 모델 저장 (최종 상태)\n",
    "model.save('resnet50_custom_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 상품 500개  학습  \n",
    "import tensorflow as tf\n",
    "# GPU가 사용 가능한지 확인하고 메모리 성장을 설정\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"사용 가능한 GPU: {gpus}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU 설정 중 오류 발생: {e}\")\n",
    "else:\n",
    "    print(\"GPU를 찾을 수 없습니다. CPU로 실행합니다.\")\n",
    "\n",
    "\n",
    "# 데이터셋 경로 설정\n",
    "train_dir = r'E:\\musinsa\\data_file\\train_product'\n",
    "val_dir = r'E:\\musinsa\\data_file\\val_product'\n",
    "\n",
    "# 이미지 크기와 배치 크기 설정\n",
    "img_height, img_width = 224, 224\n",
    "batch_size = 32\n",
    "\n",
    "# 데이터 증강 및 전처리 설정\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    fill_mode='nearest',\n",
    "    brightness_range=[0.7, 1.2],\n",
    "    channel_shift_range=0.2,\n",
    "    rescale=1./255,\n",
    "    dtype='float32'\n",
    ")\n",
    "\n",
    "# 검증 데이터는 증강하지 않음\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# 훈련 데이터 로드\n",
    "train_ds = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# 검증 데이터 로드\n",
    "val_ds = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# 증강된 이미지 확인\n",
    "x_batch, y_batch = next(train_ds)\n",
    "for i in range(5):\n",
    "    plt.figure(figsize=(2, 2))\n",
    "    plt.imshow(x_batch[i])\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# ResNet50 모델 로드 (사전 학습된 ImageNet 가중치 사용)\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "\n",
    "# ResNet50 모델의 모든 층을 학습 불가능하게 설정\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False       #False 설정하면 모든 레이어가 고정(freeze)\n",
    "\n",
    "# Sequential 모델 구성\n",
    "model = Sequential()\n",
    "\n",
    "# ResNet50 기본 모델 추가\n",
    "model.add(base_model)\n",
    "\n",
    "# 추가적인 CNN 계층 추가\n",
    "model.add(Conv2D(16, 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(32, 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(padding='same'))\n",
    "\n",
    "model.add(Conv2D(64, 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# 분류기 계층 추가\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "\n",
    "# 출력층: 클래스 수를 자동으로 설정\n",
    "model.add(Dense(train_ds.num_classes, activation='softmax'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 체크포인트 콜백 설정\n",
    "checkpoint_cb = ModelCheckpoint(r'E:\\musinsa\\checkpoints\\resnet50_custom_model_V3_checkpoint_epoch_{epoch:02d}.h5', \n",
    "                                save_best_only=True,\n",
    "                                monitor='accuracy', \n",
    "                                mode='max',\n",
    "                                verbose=1)\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    epochs=20,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[checkpoint_cb]\n",
    ")\n",
    "\n",
    "# 학습 완료 후 모델 저장 (최종 상태)\n",
    "model.save('resnet50_custom_model_500class.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#추가학습  500class \n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"사용 가능한 GPU: {gpus}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU 설정 중 오류 발생: {e}\")\n",
    "else:\n",
    "    print(\"GPU를 찾을 수 없습니다. CPU로 실행합니다.\")\n",
    "\n",
    "\n",
    "# ResNet50 모델의 모든 층을 학습 불가능하게 설정\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False       #False 설정하면 모든 레이어가 고정(freeze)\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "# 체크포인트 콜백 설정 (계속해서 체크포인트 저장)\n",
    "checkpoint_cb = ModelCheckpoint(r'E:\\musinsa\\checkpoints\\resnet50_custom_model_500class_checkpoint_epoch_{epoch:02d}.h5', \n",
    "                                save_best_only=True,\n",
    "                                monitor='accuracy', \n",
    "                                mode='max',\n",
    "                                verbose=1)\n",
    "\n",
    "# 데이터셋 경로 설정\n",
    "train_dir = r'E:\\musinsa\\data_file\\train_product'\n",
    "val_dir = r'E:\\musinsa\\data_file\\val_product'\n",
    "\n",
    "# 이미지 크기와 배치 크기 설정\n",
    "img_height, img_width = 224, 224\n",
    "batch_size = 32\n",
    "\n",
    "# 데이터 증강 및 전처리 설정\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    fill_mode='nearest',\n",
    "    brightness_range=[0.7, 1.2],\n",
    "    channel_shift_range=0.2,\n",
    "    rescale=1./255,\n",
    "    dtype='float32'\n",
    ")\n",
    "\n",
    "# 검증 데이터는 증강하지 않음\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# 훈련 데이터 로드\n",
    "train_ds = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# 검증 데이터 로드\n",
    "val_ds = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "# 추가 학습 수행\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    epochs=100,  # 남은 에폭만큼 학습 (예: 7번째 에폭까지 했으므로, 8~20 에폭 추가)\n",
    "    initial_epoch=7,  # 이전 학습이 종료된 에폭 번호를 지정\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[checkpoint_cb]\n",
    ")\n",
    "\n",
    "# 학습 완료 후 모델 저장 (최종 상태)\n",
    "model.save('resnet50_custom_model_500class_final.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습시킨 모델 불러오기 \n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model(r'E:\\musinsa\\checkpoints\\resnet50_custom_model_V3_checkpoint_epoch_05.h5')\n",
    "model.get_config() #기본값 확인 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###softmax로 반환된 확률 테스트 -상위 몇개 할지 (상품 1000개 모델 )\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import os\n",
    "\n",
    "# GPU가 사용 가능한지 확인하고 메모리 성장을 설정\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"사용 가능한 GPU: {gpus}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU 설정 중 오류 발생: {e}\")\n",
    "else:\n",
    "    print(\"GPU를 찾을 수 없습니다. CPU로 실행합니다.\")\n",
    "\n",
    "# 경로 설정\n",
    "data_dir = r'E:\\musinsa\\data_file\\train\\augmented'\n",
    "val_dir = r'E:\\musinsa\\data_file\\test'\n",
    "\n",
    "# 폴더 이름(클래스 이름) 가져오기 및 정렬\n",
    "class_names = sorted(os.listdir(data_dir))\n",
    "\n",
    "# 클래스 이름과 라벨 매핑 확인\n",
    "print(f'Class names: {class_names}')\n",
    "\n",
    "# 모델 로드 (이미 학습된 모델이 있다고 가정)\n",
    "model = tf.keras.models.load_model(r'E:\\musinsa\\checkpoints\\resnet50_custom_model_checkpoint_epoch_18.h5')\n",
    "\n",
    "# 정확도 계산을 위한 변수 초기화\n",
    "total_images = 0\n",
    "top_1_correct = 0\n",
    "top_5_correct = 0\n",
    "top_10_correct = 0\n",
    "top_15_correct = 0\n",
    "top_20_correct = 0\n",
    "top_30_correct = 0\n",
    "\n",
    "# val_10 디렉토리의 모든 하위 폴더를 순회하며 이미지 처리\n",
    "for class_folder in os.listdir(val_dir):\n",
    "    folder_path = os.path.join(val_dir, class_folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nProcessing images in folder: {class_folder}\")\n",
    "\n",
    "    for img_file in os.listdir(folder_path):\n",
    "        total_images += 1\n",
    "        image_path = os.path.join(folder_path, img_file)\n",
    "        \n",
    "        # 이미지 로드 및 전처리\n",
    "        img = image.load_img(image_path, target_size=(224, 224))  # 이미지 크기 모델에 맞게 조정\n",
    "        img_array = image.img_to_array(img)  # 이미지를 배열로 변환\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # 배치 차원 추가\n",
    "        img_array = tf.keras.applications.resnet50.preprocess_input(img_array)  # ResNet50 전처리\n",
    "        \n",
    "        # 예측\n",
    "        predictions = model.predict(img_array)\n",
    "        \n",
    "        # 상위 5개의 클래스 인덱스와 확률 추출\n",
    "        top_5_indices = np.argsort(predictions[0])[::-1][:5]\n",
    "        top_5_probabilities = predictions[0][top_5_indices]\n",
    "        top_5_class_names = [class_names[i] for i in top_5_indices]\n",
    "        # 상위 10개의 클래스 인덱스와 확률 추출\n",
    "        top_10_indices = np.argsort(predictions[0])[::-1][:10]\n",
    "        top_10_probabilities = predictions[0][top_10_indices]\n",
    "        top_10_class_names = [class_names[i] for i in top_10_indices]\n",
    "\n",
    "        # 상위 15개의 클래스 인덱스와 확률 추출\n",
    "        top_15_indices = np.argsort(predictions[0])[::-1][:15]\n",
    "        top_15_probabilities = predictions[0][top_15_indices]\n",
    "        top_15_class_names = [class_names[i] for i in top_15_indices]\n",
    "\n",
    "        # 상위 20개의 클래스 인덱스와 확률 추출\n",
    "        top_20_indices = np.argsort(predictions[0])[::-1][:20]\n",
    "        top_20_probabilities = predictions[0][top_20_indices]\n",
    "        top_20_class_names = [class_names[i] for i in top_20_indices]   \n",
    "        # 상위 30개의 클래스 인덱스와 확률 추출\n",
    "        top_30_indices = np.argsort(predictions[0])[::-1][:20]\n",
    "        top_30_probabilities = predictions[0][top_30_indices]\n",
    "        top_30_class_names = [class_names[i] for i in top_30_indices]       \n",
    "\n",
    "        # 결과 출력\n",
    "        print(f'Image: {img_file}')\n",
    "        for i in range(5):\n",
    "            print(f\"Top {i+1} prediction: {top_5_class_names[i]} (Probability: {top_5_probabilities[i]:.4f})\")\n",
    "        \n",
    "        # 정확도 계산\n",
    "        if class_folder == top_5_class_names[0]:\n",
    "            top_1_correct += 1\n",
    "        if class_folder in top_5_class_names:\n",
    "            top_5_correct += 1\n",
    "        if class_folder in top_10_class_names:\n",
    "            top_10_correct += 1\n",
    "        if class_folder in top_15_class_names:\n",
    "            top_15_correct += 1\n",
    "        if class_folder in top_20_class_names:\n",
    "            top_20_correct += 1     \n",
    "        if class_folder in top_30_class_names:\n",
    "            top_20_correct += 1     \n",
    "        # 실제 폴더 이름과 상위 예측 클래스 비교\n",
    "        match = class_folder in top_5_class_names\n",
    "        print(f'Match with folder name in top 5 predictions: {match}')  # True if folder name is in top 5 predictions\n",
    "# 최종 정확도 계산\n",
    "top_1_accuracy = top_1_correct / total_images\n",
    "top_5_accuracy = top_5_correct / total_images\n",
    "top_10_accuracy = top_10_correct / total_images\n",
    "top_15_accuracy = top_15_correct / total_images\n",
    "top_20_accuracy = top_20_correct / total_images\n",
    "top_30_accuracy = top_30_correct / total_images\n",
    "\n",
    "\n",
    "print(f\"\\nTotal images: {total_images}\")\n",
    "print(f\"Top-1 Accuracy: {top_1_accuracy:.4f}\")\n",
    "print(f\"Top-5 Accuracy: {top_5_accuracy:.4f}\")\n",
    "print(f\"Top-10 Accuracy: {top_10_accuracy:.4f}\")\n",
    "print(f\"Top-15Accuracy: {top_15_accuracy:.4f}\")\n",
    "print(f\"Top-20 Accuracy: {top_20_accuracy:.4f}\")\n",
    "print(f\"Top-30 Accuracy: {top_30_accuracy:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###최종이다 -소형 모델  ##\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import os\n",
    "\n",
    "# GPU가 사용 가능한지 확인하고 메모리 성장을 설정\n",
    "# 경로 설정\n",
    "data_dir = r'E:\\musinsa\\data_file\\train\\augmented'\n",
    "val_dir = r'E:\\musinsa\\data_file\\test'\n",
    "\n",
    "# 폴더 이름(클래스 이름) 가져오기 및 정렬\n",
    "class_names = sorted(os.listdir(data_dir))\n",
    "\n",
    "# 클래스 이름과 라벨 매핑 확인\n",
    "print(f'Class names: {class_names}')\n",
    "\n",
    "# 모델 로드 (이미 학습된 모델이 있다고 가정)\n",
    "model = tf.keras.models.load_model(r'E:\\musinsa\\checkpoints\\resnet50_custom_model_V3_checkpoint_epoch_05.h5')\n",
    "\n",
    "# 정확도 계산을 위한 변수 초기화\n",
    "total_images = 0\n",
    "top_1_correct = 0\n",
    "top_5_correct = 0\n",
    "\n",
    "\n",
    "# val_10 디렉토리의 모든 하위 폴더를 순회하며 이미지 처리\n",
    "for class_folder in os.listdir(val_dir):\n",
    "    folder_path = os.path.join(val_dir, class_folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nProcessing images in folder: {class_folder}\")\n",
    "\n",
    "    for img_file in os.listdir(folder_path):\n",
    "        total_images += 1\n",
    "        image_path = os.path.join(folder_path, img_file)\n",
    "        \n",
    "        # 이미지 로드 및 전처리\n",
    "        img = image.load_img(image_path, target_size=(224, 224))  # 이미지 크기 모델에 맞게 조정\n",
    "        img_array = image.img_to_array(img)  # 이미지를 배열로 변환\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # 배치 차원 추가\n",
    "        img_array = tf.keras.applications.resnet50.preprocess_input(img_array)  # ResNet50 전처리\n",
    "        \n",
    "        # 예측\n",
    "        predictions = model.predict(img_array)\n",
    "        \n",
    "        # 상위 5개의 클래스 인덱스와 확률 추출\n",
    "        top_5_indices = np.argsort(predictions[0])[::-1][:5]\n",
    "        top_5_probabilities = predictions[0][top_5_indices]\n",
    "        top_5_class_names = [class_names[i] for i in top_5_indices]    \n",
    "        # 결과 출력\n",
    "        print(f'Image: {img_file}')\n",
    "        for i in range(5):\n",
    "            print(f\"Top {i+1} prediction: {top_5_class_names[i]} (Probability: {top_5_probabilities[i]:.4f})\")\n",
    "        \n",
    "        # 정확도 계산\n",
    "        if class_folder == top_5_class_names[0]:\n",
    "            top_1_correct += 1\n",
    "        if class_folder in top_5_class_names:\n",
    "            top_5_correct += 1\n",
    "        # 실제 폴더 이름과 상위 예측 클래스 비교\n",
    "        match = class_folder in top_5_class_names\n",
    "        print(f'Match with folder name in top 5 predictions: {match}')  # True if folder name is in top 5 predictions\n",
    "# 최종 정확도 계산\n",
    "top_1_accuracy = top_1_correct / total_images\n",
    "top_5_accuracy = top_5_correct / total_images\n",
    "\n",
    "\n",
    "print(f\"\\nTotal images: {total_images}\")\n",
    "print(f\"Top-1 Accuracy: {top_1_accuracy:.4f}\")\n",
    "print(f\"Top-5 Accuracy: {top_5_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
